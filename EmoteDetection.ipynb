{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4724669b-9f59-4abb-9a2a-e6149128848f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e72dad13-80b4-4f51-b720-12dc8f8bf7f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    rotation_range=30,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest'\n",
    ")\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2a5a7af1-94cf-4490-a685-97a5d14d133b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 28709 images belonging to 7 classes.\n",
      "Found 7178 images belonging to 7 classes.\n"
     ]
    }
   ],
   "source": [
    "train_generator = train_datagen.flow_from_directory(\n",
    "    'train',\n",
    "    target_size=(48, 48),\n",
    "    batch_size=64,\n",
    "    color_mode='grayscale',\n",
    "    class_mode='categorical'\n",
    ")\n",
    "\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "    'test',\n",
    "    target_size=(48, 48),\n",
    "    batch_size=64,\n",
    "    color_mode='grayscale',\n",
    "    class_mode='categorical'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90a15fc2-5ae3-4537-9495-b3432dd79088",
   "metadata": {},
   "source": [
    "## Introdução\n",
    "As Redes Neurais Convolucionais (CNNs) são um tipo de rede neural projetada para lidar com imagens. Elas são amplamente utilizadas em tarefas como classificação de imagens, detecção de objetos e reconhecimento facial, pois são ótimas em identificar padrões visuais."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d494d4f-c9b0-4f25-8ead-05086d5f4e3a",
   "metadata": {},
   "source": [
    "### Estrutura de uma CNN\n",
    "Uma CNN é composta de três tipos principais de camadas:\n",
    "\n",
    "- Camada Convolucional (Conv2D)\n",
    "- Camada de Pooling (MaxPooling2D)\n",
    "- Camada Densa"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1899b572-a51c-4e0b-bac4-78ebb772c5e6",
   "metadata": {},
   "source": [
    "<img src=\"CNN Structure.PNG\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9fc01e2-3eac-419b-9760-7327d215e235",
   "metadata": {},
   "source": [
    "#### Camada Convolucional\n",
    "\n",
    "A camada convolucional é a mais importante em uma CNN. Seu objetivo é extrair características da imagem, como bordas, texturas ou formas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "552c22e8-6427-4df4-9457-d3cd601cbef0",
   "metadata": {},
   "source": [
    "#### Camada de Pooling\n",
    "\n",
    "A camada de pooling reduz a dimensionalidade dos mapas de características. Isso ajuda a compactar a informação e reduzir a quantidade de parâmetros, o que acelera o treinamento e evita o overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38c147bd-48cf-440c-88ee-4821feb8cd2e",
   "metadata": {},
   "source": [
    "#### Camada Densa\n",
    "\n",
    "A camada densa funciona como em redes neurais tradicionais, onde cada neurônio está conectado a todos os outros. Ela usa as características extraídas para fazer a previsão final."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3445f265-7d7e-4fee-904d-ce61620454c3",
   "metadata": {},
   "source": [
    "<center><br> <h2>Funcionamento da Camada Convolucional. </h2><br>\n",
    "<img src=\"convolucao_gato.png\"></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e6843054-4280-4d5f-816a-e973734be8de",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Conv2D(32, (3, 3), activation='relu', input_shape=(48, 48, 1)),\n",
    "    tf.keras.layers.MaxPooling2D(2, 2),\n",
    "    \n",
    "    tf.keras.layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D(2, 2),\n",
    "\n",
    "    tf.keras.layers.Conv2D(128, (3, 3), activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D(2, 2),\n",
    "    \n",
    "    tf.keras.layers.Conv2D(256, (3, 3), activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D(2, 2),\n",
    "\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(128, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.5),\n",
    "    tf.keras.layers.Dense(7, activation='softmax')\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "62a9ad49-6385-463a-9b91-fb4b9160ba84",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "001324f1-c0bf-4034-9bdb-73ecd43035ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "449/449 [==============================] - 66s 144ms/step - loss: 1.8184 - accuracy: 0.2479 - val_loss: 1.7917 - val_accuracy: 0.2540\n",
      "Epoch 2/50\n",
      "449/449 [==============================] - 41s 90ms/step - loss: 1.7848 - accuracy: 0.2555 - val_loss: 1.7194 - val_accuracy: 0.2923\n",
      "Epoch 3/50\n",
      "449/449 [==============================] - 43s 96ms/step - loss: 1.7503 - accuracy: 0.2796 - val_loss: 1.6838 - val_accuracy: 0.3168\n",
      "Epoch 4/50\n",
      "449/449 [==============================] - 41s 91ms/step - loss: 1.7011 - accuracy: 0.3059 - val_loss: 1.5443 - val_accuracy: 0.4074\n",
      "Epoch 5/50\n",
      "449/449 [==============================] - 41s 91ms/step - loss: 1.6128 - accuracy: 0.3631 - val_loss: 1.4181 - val_accuracy: 0.4639\n",
      "Epoch 6/50\n",
      "449/449 [==============================] - 41s 92ms/step - loss: 1.5361 - accuracy: 0.3985 - val_loss: 1.3588 - val_accuracy: 0.4857\n",
      "Epoch 7/50\n",
      "449/449 [==============================] - 42s 93ms/step - loss: 1.4882 - accuracy: 0.4220 - val_loss: 1.3174 - val_accuracy: 0.4896\n",
      "Epoch 8/50\n",
      "449/449 [==============================] - 47s 105ms/step - loss: 1.4600 - accuracy: 0.4378 - val_loss: 1.2839 - val_accuracy: 0.5061\n",
      "Epoch 9/50\n",
      "449/449 [==============================] - 41s 90ms/step - loss: 1.4315 - accuracy: 0.4512 - val_loss: 1.2838 - val_accuracy: 0.5070\n",
      "Epoch 10/50\n",
      "449/449 [==============================] - 51s 114ms/step - loss: 1.4145 - accuracy: 0.4570 - val_loss: 1.2365 - val_accuracy: 0.5228\n",
      "Epoch 11/50\n",
      "449/449 [==============================] - 41s 91ms/step - loss: 1.3892 - accuracy: 0.4694 - val_loss: 1.2343 - val_accuracy: 0.5298\n",
      "Epoch 12/50\n",
      "449/449 [==============================] - 42s 93ms/step - loss: 1.3753 - accuracy: 0.4749 - val_loss: 1.2138 - val_accuracy: 0.5327\n",
      "Epoch 13/50\n",
      "449/449 [==============================] - 44s 98ms/step - loss: 1.3646 - accuracy: 0.4807 - val_loss: 1.1983 - val_accuracy: 0.5398\n",
      "Epoch 14/50\n",
      "449/449 [==============================] - 45s 100ms/step - loss: 1.3498 - accuracy: 0.4835 - val_loss: 1.2077 - val_accuracy: 0.5398\n",
      "Epoch 15/50\n",
      "449/449 [==============================] - 41s 92ms/step - loss: 1.3403 - accuracy: 0.4914 - val_loss: 1.1820 - val_accuracy: 0.5461\n",
      "Epoch 16/50\n",
      "449/449 [==============================] - 47s 105ms/step - loss: 1.3325 - accuracy: 0.4944 - val_loss: 1.1668 - val_accuracy: 0.5543\n",
      "Epoch 17/50\n",
      "449/449 [==============================] - 48s 107ms/step - loss: 1.3217 - accuracy: 0.4968 - val_loss: 1.1652 - val_accuracy: 0.5578\n",
      "Epoch 18/50\n",
      "449/449 [==============================] - 44s 98ms/step - loss: 1.3073 - accuracy: 0.5042 - val_loss: 1.1671 - val_accuracy: 0.5527\n",
      "Epoch 19/50\n",
      "449/449 [==============================] - 49s 108ms/step - loss: 1.3056 - accuracy: 0.5045 - val_loss: 1.1653 - val_accuracy: 0.5627\n",
      "Epoch 20/50\n",
      "449/449 [==============================] - 42s 93ms/step - loss: 1.2989 - accuracy: 0.5059 - val_loss: 1.1692 - val_accuracy: 0.5552\n",
      "Epoch 21/50\n",
      "449/449 [==============================] - 47s 104ms/step - loss: 1.2928 - accuracy: 0.5072 - val_loss: 1.1505 - val_accuracy: 0.5587\n",
      "Epoch 22/50\n",
      "449/449 [==============================] - 45s 100ms/step - loss: 1.2856 - accuracy: 0.5152 - val_loss: 1.1473 - val_accuracy: 0.5584\n",
      "Epoch 23/50\n",
      "449/449 [==============================] - 41s 91ms/step - loss: 1.2823 - accuracy: 0.5156 - val_loss: 1.1483 - val_accuracy: 0.5589\n",
      "Epoch 24/50\n",
      "449/449 [==============================] - 43s 97ms/step - loss: 1.2786 - accuracy: 0.5182 - val_loss: 1.1751 - val_accuracy: 0.5605\n",
      "Epoch 25/50\n",
      "449/449 [==============================] - 40s 90ms/step - loss: 1.2695 - accuracy: 0.5210 - val_loss: 1.1316 - val_accuracy: 0.5680\n",
      "Epoch 26/50\n",
      "449/449 [==============================] - 40s 89ms/step - loss: 1.2663 - accuracy: 0.5198 - val_loss: 1.1444 - val_accuracy: 0.5644\n",
      "Epoch 27/50\n",
      "449/449 [==============================] - 43s 95ms/step - loss: 1.2620 - accuracy: 0.5237 - val_loss: 1.2015 - val_accuracy: 0.5451\n",
      "Epoch 28/50\n",
      "449/449 [==============================] - 47s 104ms/step - loss: 1.2572 - accuracy: 0.5233 - val_loss: 1.1246 - val_accuracy: 0.5729\n",
      "Epoch 29/50\n",
      "449/449 [==============================] - 45s 100ms/step - loss: 1.2549 - accuracy: 0.5274 - val_loss: 1.1244 - val_accuracy: 0.5683\n",
      "Epoch 30/50\n",
      "449/449 [==============================] - 46s 102ms/step - loss: 1.2487 - accuracy: 0.5288 - val_loss: 1.1261 - val_accuracy: 0.5702\n",
      "Epoch 31/50\n",
      "449/449 [==============================] - 46s 102ms/step - loss: 1.2469 - accuracy: 0.5303 - val_loss: 1.1135 - val_accuracy: 0.5819\n",
      "Epoch 32/50\n",
      "449/449 [==============================] - 49s 110ms/step - loss: 1.2489 - accuracy: 0.5295 - val_loss: 1.1313 - val_accuracy: 0.5737\n",
      "Epoch 33/50\n",
      "449/449 [==============================] - 43s 95ms/step - loss: 1.2413 - accuracy: 0.5322 - val_loss: 1.1225 - val_accuracy: 0.5663\n",
      "Epoch 34/50\n",
      "449/449 [==============================] - 41s 92ms/step - loss: 1.2463 - accuracy: 0.5275 - val_loss: 1.1223 - val_accuracy: 0.5726\n",
      "Epoch 35/50\n",
      "449/449 [==============================] - 40s 89ms/step - loss: 1.2367 - accuracy: 0.5307 - val_loss: 1.1248 - val_accuracy: 0.5748\n",
      "Epoch 36/50\n",
      "449/449 [==============================] - 41s 92ms/step - loss: 1.2292 - accuracy: 0.5340 - val_loss: 1.1091 - val_accuracy: 0.5763\n",
      "Epoch 37/50\n",
      "449/449 [==============================] - 44s 98ms/step - loss: 1.2283 - accuracy: 0.5370 - val_loss: 1.1174 - val_accuracy: 0.5756\n",
      "Epoch 38/50\n",
      "449/449 [==============================] - 49s 110ms/step - loss: 1.2214 - accuracy: 0.5389 - val_loss: 1.0983 - val_accuracy: 0.5782\n",
      "Epoch 39/50\n",
      "449/449 [==============================] - 46s 102ms/step - loss: 1.2187 - accuracy: 0.5420 - val_loss: 1.1127 - val_accuracy: 0.5804\n",
      "Epoch 40/50\n",
      "449/449 [==============================] - 47s 105ms/step - loss: 1.2216 - accuracy: 0.5384 - val_loss: 1.0902 - val_accuracy: 0.5882\n",
      "Epoch 41/50\n",
      "449/449 [==============================] - 46s 101ms/step - loss: 1.2150 - accuracy: 0.5424 - val_loss: 1.0901 - val_accuracy: 0.5876\n",
      "Epoch 42/50\n",
      "283/449 [=================>............] - ETA: 14s - loss: 1.2177 - accuracy: 0.5409"
     ]
    }
   ],
   "source": [
    "model.fit(\n",
    "    train_generator,\n",
    "    epochs=50,\n",
    "    validation_data=test_generator\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "280b2476-f4e0-4f0c-99c1-4c90c73c9f85",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('fer_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e848622b-b3b9-42aa-a67e-a143ff0fa6d2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
